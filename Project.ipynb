{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, TensorDataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# from models import build_or_load_gen_model\n",
    "# from evaluator import smooth_bleu\n",
    "# from evaluator.CodeBLEU import calc_code_bleu\n",
    "# from evaluator.bleu import _bleu\n",
    "# from utils import get_filenames, get_elapse_time, load_and_cache_gen_data\n",
    "# from configs import add_args, set_seed, set_dist\n",
    "\n",
    "from transformers import (RobertaTokenizer, T5Config, T5ForConditionalGeneration, \n",
    "                          AdamW, get_linear_schedule_with_warmup)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Class Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Class ():\n",
    "    def __init__(self, config):\n",
    "        self.__dict__ = config\n",
    "        self.args = self.__dict__\n",
    "        self.load_writers()\n",
    "        self.load_model()\n",
    "        self.train_filename = f\"{self.data_dir}/{self.train_filename}\"\n",
    "        self.dev_filename = f\"{self.data_dir}/{self.dev_filename}\"\n",
    "        self.test_filename = f\"{self.data_dir}/{self.test_filename}\"\n",
    "\n",
    "    def convert_examples_to_features(self, item):\n",
    "        example, example_index, tokenizer, args, stage = item\n",
    "\n",
    "        source_str = \"{}: {}\".format(args.task, example.source)\n",
    "\n",
    "        source_str = source_str.replace('</s>', '<unk>')\n",
    "        source_ids = tokenizer.encode(source_str, max_length=args.max_source_length, padding='max_length', truncation=True)\n",
    "        assert source_ids.count(tokenizer.eos_token_id) == 1\n",
    "        if stage == 'test':\n",
    "            target_ids = []\n",
    "        else:\n",
    "            target_str = example.target\n",
    "            target_str = target_str.replace('</s>', '<unk>')\n",
    "            target_ids = tokenizer.encode(target_str, max_length=args.max_target_length, padding='max_length',\n",
    "                                        truncation=True)\n",
    "            assert target_ids.count(tokenizer.eos_token_id) == 1\n",
    "\n",
    "        return InputFeatures(\n",
    "            example_index,\n",
    "            source_ids,\n",
    "            target_ids,\n",
    "            url=example.url\n",
    "        )\n",
    "    \n",
    "    def save_model_state(self, location):\n",
    "        torch.save(self.model.state_dict(), f\"{self.output_dir}/{location}\")\n",
    "\n",
    "    def training_loop(self):\n",
    "        # Load Train DataLoader\n",
    "        train_examples, train_data = self.load_data(filename  = self.train_filename, \n",
    "                                    split_tag = 'train')\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, \n",
    "                                      sampler=train_sampler, \n",
    "                                      batch_size=self.train_batch_size,\n",
    "                                      num_workers=4, \n",
    "                                      pin_memory=True)\n",
    "        \n",
    "        # Load Optimizer and Scheduler\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "            'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=float(self.learning_rate), eps=float(self.adam_epsilon))\n",
    "        num_train_optimization_steps = self.num_train_epochs * len(train_dataloader)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=self.warmup_steps,\n",
    "                                                    num_training_steps=num_train_optimization_steps)\n",
    "        # Log Info\n",
    "        train_example_num = len(train_data)\n",
    "        self.log_info(1, \"***** Running training *****\")\n",
    "        self.log_info(1, \"  Num examples = %d\", train_example_num)\n",
    "        self.log_info(1, \"  Batch size = %d\", self.train_batch_size)\n",
    "        self.log_info(1, \"  Batch num = %d\", math.ceil(train_example_num / self.train_batch_size))\n",
    "        self.log_info(1, \"  Num epoch = %d\", self.num_train_epochs)\n",
    "        \n",
    "        # Parameters\n",
    "        self.dev_dataset = {}\n",
    "        self.global_step, best_bleu_em, self.best_ppl = 0, -1, 1e6\n",
    "        self.not_loss_dec_cnt, self.not_bleu_em_inc_cnt = 0, 0 if self.do_eval_bleu else 1e6\n",
    "\n",
    "        # Main Training Loop\n",
    "        for cur_epoch in range(self.start_epoch, int(self.num_train_epochs)):\n",
    "            bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "            nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "            self.model.train()\n",
    "            for step, batch in enumerate(bar):\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                source_ids, target_ids = batch\n",
    "                source_mask = source_ids.ne(self.tokenizer.pad_token_id)\n",
    "                target_mask = target_ids.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "                self.model.config.decoder_start_token_id = self.decoder_start_token_id\n",
    "                outputs = self.model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                     labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                nb_tr_examples += source_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                loss.backward()\n",
    "\n",
    "                if nb_tr_steps % self.gradient_accumulation_steps == 0:\n",
    "                    # Update parameters\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "                    self.global_step += 1\n",
    "                    train_loss = round(tr_loss * self.gradient_accumulation_steps / (nb_tr_steps + 1), 4)\n",
    "                    bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "            if self.do_eval :\n",
    "                self.eval(cur_epoch)\n",
    "                if self.do_eval_bleu :\n",
    "                    self.eval_bleu()\n",
    "\n",
    "    def eval_bleu(self):\n",
    "        # if args.do_eval_bleu:\n",
    "        #     eval_examples, eval_data = load_and_cache_gen_data(args, args.dev_filename, pool, tokenizer, 'dev',\n",
    "        #                                                         only_src=True, is_sample=True)\n",
    "\n",
    "        #     result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'dev', 'e%d' % cur_epoch)\n",
    "        #     dev_bleu, dev_em = result['bleu'], result['em']\n",
    "        #     if args.task in ['summarize']:\n",
    "        #         dev_bleu_em = dev_bleu\n",
    "        #     elif args.task in ['defect']:\n",
    "        #         dev_bleu_em = dev_em\n",
    "        #     else:\n",
    "        #         dev_bleu_em = dev_bleu + dev_em\n",
    "        #     if args.data_num == -1:\n",
    "        #         tb_writer.add_scalar('dev_bleu_em', dev_bleu_em, cur_epoch)\n",
    "        #         # tb_writer.add_scalar('dev_em', dev_em, cur_epoch)\n",
    "        #     if dev_bleu_em > best_bleu_em:\n",
    "        #         self.not_bleu_em_inc_cnt = 0\n",
    "        #         logger.info(\"  [%d] Best bleu+em: %.2f (bleu: %.2f, em: %.2f)\",\n",
    "        #                     cur_epoch, dev_bleu_em, dev_bleu, dev_em)\n",
    "        #         logger.info(\"  \" + \"*\" * 20)\n",
    "        #         best_bleu_em = dev_bleu_em\n",
    "        #         fa.write(\"[%d] Best bleu+em changed into %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "        #             cur_epoch, best_bleu_em, dev_bleu, dev_em))\n",
    "        #         # Save best checkpoint for best bleu\n",
    "        #         output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
    "        #         if not os.path.exists(output_dir):\n",
    "        #             os.makedirs(output_dir)\n",
    "        #         if args.data_num == -1 or args.always_save_model:\n",
    "        #             model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        #             output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "        #             torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        #             logger.info(\"Save the best bleu model into %s\", output_model_file)\n",
    "        #     else:\n",
    "        #         self.not_bleu_em_inc_cnt += 1\n",
    "        #         logger.info(\"Bleu does not increase for %d epochs\", self.not_bleu_em_inc_cnt)\n",
    "        #         fa.write(\n",
    "        #             \"[%d] Best bleu+em (%.2f) does not drop changed for %d epochs, cur bleu+em: %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "        #                 cur_epoch, best_bleu_em, self.not_bleu_em_inc_cnt, dev_bleu_em, dev_bleu, dev_em))\n",
    "        #         if all([x > args.patience for x in [ self.not_bleu_em_inc_cnt, self.not_loss_dec_cnt]]):\n",
    "        #             stop_early_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and self.not_loss_dec_cnt=%d\\n\" % (\n",
    "        #                 cur_epoch, self.not_bleu_em_inc_cnt, self.not_loss_dec_cnt)\n",
    "        #             logger.info(stop_early_str)\n",
    "        #             fa.write(stop_early_str)\n",
    "        #             break\n",
    "        self.log_info(1, \"eval-bleu needs to be added\")\n",
    "\n",
    "    def eval(self, cur_epoch):\n",
    "        # Eval model with dev dataset\n",
    "        if 'dev_loss' in self.dev_dataset:\n",
    "            eval_examples, eval_data = self.dev_dataset['dev_loss']\n",
    "        else:            \n",
    "            eval_examples, eval_data = self.load_data(filename=self.dev_filename, \n",
    "                                                      split_tag='dev')\n",
    "            self.dev_dataset['dev_loss'] = eval_examples, eval_data\n",
    "\n",
    "        eval_ppl = self.eval_ppl_epoch(eval_data, eval_examples)\n",
    "        result = {'epoch': cur_epoch, 'global_step': self.global_step, 'eval_ppl': eval_ppl}\n",
    "        for key in sorted(result.keys()):\n",
    "            self.log_info(1, \"  %s = %s\", key, str(result[key]))\n",
    "        self.log_info(1, \"  \" + \"*\" * 20)\n",
    "        # if self.data_num == -1:\n",
    "            # tb_writer.add_scalar('dev_ppl', eval_ppl, cur_epoch)\n",
    "\n",
    "        if eval_ppl < self.best_ppl:\n",
    "            self.not_loss_dec_cnt = 0\n",
    "            self.log_info(1, \"  Best ppl:%s\", eval_ppl)\n",
    "            self.log_info(1, \"  \" + \"*\" * 20)\n",
    "            # fa.write(\"[%d] Best ppl changed into %.4f\\n\" % (cur_epoch, eval_ppl))\n",
    "            self.best_ppl = eval_ppl\n",
    "\n",
    "            # Save best checkpoint for best ppl\n",
    "            output_dir = os.path.join(self.output_dir, 'checkpoint-best-ppl')\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            if self.always_save_model:\n",
    "                model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "                output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                self.log_info(1, \"Save the best ppl model into %s\", output_model_file)\n",
    "        else:\n",
    "            self.not_loss_dec_cnt += 1\n",
    "            self.log_info(1, \"Ppl does not decrease for %d epochs\",self.not_loss_dec_cnt)\n",
    "            if all([x > self.patience for x in [ self.not_bleu_em_inc_cnt,self.not_loss_dec_cnt]]):\n",
    "                early_stop_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and not_loss_dec_cnt=%d\\n\" % (\n",
    "                    cur_epoch, self.not_bleu_em_inc_cnt, self.not_loss_dec_cnt)\n",
    "                self.log_info(1, early_stop_str)\n",
    "                # fa.write(early_stop_str)\n",
    "                # return(break)\n",
    "        self.log_info(1, \"***** CUDA.empty_cache() *****\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def read_concode_examples(self, filename, data_num):\n",
    "        \"\"\"Read examples from filename.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        with open(filename) as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                x = json.loads(line)\n",
    "                examples.append(\n",
    "                    Example(\n",
    "                        idx=idx,\n",
    "                        source=x[\"nl\"].strip(),\n",
    "                        target=x[\"code\"].strip()\n",
    "                    )\n",
    "                )\n",
    "                idx += 1\n",
    "                if idx == data_num:\n",
    "                    break\n",
    "        return examples\n",
    "\n",
    "    def load_data(self, filename, split_tag):\n",
    "        data_tag = '_all' if self.data_num == -1 else '_%d' % self.data_num\n",
    "        cache_fn = '{}/{}.pt'.format(self.cache_path, split_tag + data_tag)\n",
    "\n",
    "        examples = self.read_concode_examples(filename, self.data_num)\n",
    "\n",
    "        if self.is_sample:\n",
    "            examples = random.sample(examples, min(self.sample_size, len(examples)))\n",
    "        # if split_tag == 'train':\n",
    "        #     calc_stats(examples, self.tokenizer, is_tokenize=True)\n",
    "        # else:\n",
    "        #     calc_stats(examples)\n",
    "        if os.path.exists(cache_fn) and not self.is_sample:\n",
    "            self.log_info(1, \"Load cache data from %s\", cache_fn)\n",
    "            data = torch.load(cache_fn)\n",
    "        else:\n",
    "            if self.is_sample:\n",
    "                self.log_info(1, \"Sample 5k data for computing bleu from %s\", filename)\n",
    "            else:\n",
    "                self.log_info(1, \"Create cache data into %s\", cache_fn)\n",
    "            tuple_examples = [(example, idx, self.tokenizer, self, split_tag) for idx, example in enumerate(examples)]\n",
    "            features = list(map(self.convert_examples_to_features, tqdm(tuple_examples, total=len(tuple_examples))))\n",
    "            all_source_ids = torch.tensor([f.source_ids for f in features], dtype=torch.long)\n",
    "            if split_tag == 'test':\n",
    "                data = TensorDataset(all_source_ids)\n",
    "            else:\n",
    "                all_target_ids = torch.tensor([f.target_ids for f in features], dtype=torch.long)\n",
    "                data = TensorDataset(all_source_ids, all_target_ids)\n",
    "            if not self.is_sample:\n",
    "                torch.save(data, cache_fn)\n",
    "        return examples, data\n",
    "\n",
    "    def get_model_size(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return \"{}M\".format(round(model_size / 1e+6))\n",
    "\n",
    "    def load_model(self, config = None, tokenizer = None, model = None, expirement = False):\n",
    "        self.config = T5Config.from_pretrained(self.config_name) if config is None else config\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.tokenizer_name) if tokenizer is None else tokenizer\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name) if model is None else model\n",
    "        self.log_info(0,\"Finish loading model [%s] from %s\", self.get_model_size(), self.model_name)\n",
    "        if self.load_model_path is not None and not expirement:\n",
    "            self.log_info(0, \"Reload model from {}\".format(self.load_model_path))\n",
    "            self.model.load_state_dict(torch.load(self.load_model_path))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def load_writers(self):\n",
    "        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.summary_fn = '{}/{}'.format(self.summary_dir, '/'.join(self.output_dir.split('/')[1:]))\n",
    "        self.tb_writer = SummaryWriter(self.summary_fn)\n",
    "\n",
    "    def log_info(self, level, information,*args):\n",
    "        if (self.log_verbose and level <= self.info_level):\n",
    "            self.logger.info(information,*args)\n",
    "\n",
    "    def write_summary(self):\n",
    "        if self.summary_verbose:\n",
    "            print('ok') #fix\n",
    "\n",
    "    def eval_ppl_epoch(self, eval_data, eval_examples):\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.eval_batch_size,\n",
    "                                    num_workers=4, pin_memory=True)\n",
    "        # Start evaluating model\n",
    "        self.log_info(1, \"  \" + \"***** Running ppl evaluation *****\")\n",
    "        self.log_info(1, \"  Num examples = %d\", len(eval_examples))\n",
    "        self.log_info(1, \"  Batch size = %d\", self.eval_batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        eval_loss, batch_num = 0, 0\n",
    "        for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval ppl\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            source_ids, target_ids = batch\n",
    "            source_mask = source_ids.ne(self.tokenizer.pad_token_id)\n",
    "            target_mask = target_ids.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            batch_num += 1\n",
    "        eval_loss = eval_loss / batch_num\n",
    "        eval_ppl = round(np.exp(eval_loss), 5)\n",
    "        return eval_ppl\n",
    "\n",
    "    def getter_function(self, attribute):\n",
    "        return getattr(self,attribute)\n",
    "\n",
    "    def setter_function(self, attribute, value):\n",
    "        setattr(self,attribute,value)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate(self, input_sentence,min_length=20, max_length=50):\n",
    "        input = self.tokenizer.encode(input_sentence, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(input,min_length=min_length,max_length=max_length)\n",
    "        return(self.tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adam_epsilon': '1e-08',\n",
       " 'always_save_model': True,\n",
       " 'cache_path': 'cache_data',\n",
       " 'config_name': 'roberta-base',\n",
       " 'cpu_cont': 1,\n",
       " 'data_dir': 'data',\n",
       " 'data_num': -1,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'dev_filename': 'validation.json',\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'eval_batch_size': 128,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'info_level': 1,\n",
       " 'is_sample': True,\n",
       " 'learning_rate': '5e-05',\n",
       " 'load_model_path': 'saved_models/checkpoint-best-ppl/Baseline_Best_ppl.bin',\n",
       " 'log_verbose': True,\n",
       " 'max_source_length': 45,\n",
       " 'max_target_length': 30,\n",
       " 'model_name': 'roberta-base',\n",
       " 'num_train_epochs': 5,\n",
       " 'output_dir': 'saved_models/',\n",
       " 'patience': 5,\n",
       " 'sample_size': 5000,\n",
       " 'save_last_checkpoints': False,\n",
       " 'seed': 1234,\n",
       " 'start_epoch': 0,\n",
       " 'summary_dir': 'summary',\n",
       " 'summary_verbose': False,\n",
       " 'task': 'concode',\n",
       " 'test_filename': 'None',\n",
       " 'tokenizer_name': 'roberta-base',\n",
       " 'train_batch_size': 128,\n",
       " 'train_filename': 'train.json',\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = yaml.load(open(\"config.yml\"),Loader=yaml.FullLoader)\n",
    "display(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load & Train Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline = Model_Class(config)\n",
    "train_base = input(\"Would you like to train the Base_Model (y/n): \")\n",
    "if train_base == 'y':\n",
    "    Baseline.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expirements Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expirement_config = yaml.load(open(\"config.yml\"),Loader=yaml.FullLoader)\n",
    "display(expirement_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model = Model_Class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup and Apply Lora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 152,474,112 || trainable%: 0.38683550424612406\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# Hyperparameters\n",
    "r = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.1\n",
    "model = Baseline.model\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, \n",
    "                         inference_mode=False, \n",
    "                         r=r, \n",
    "                         lora_alpha=lora_alpha, \n",
    "                         lora_dropout=lora_dropout)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "Baseline.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lora = input(\"Would you like to train the Lora Model (y/n): \")\n",
    "if train_lora == 'y':\n",
    "    Baseline.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Picard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Training and Evaluation Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_fn = f\"{Baseline.cache_path}/train_all.pt\"\n",
    "\n",
    "if not os.path.exists(cache_fn):\n",
    "    Clinton_dataset = load_dataset(\"Clinton/Text-to-sql-v1\")\n",
    "    b_mc2_dataset = load_dataset(\"b-mc2/sql-create-context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 5, 1, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "list = [1,3,4,5,6]\n",
    "random.shuffle(list)\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Question\n",
      "Write an SQL query that answers this question:\n",
      "What was the score of the game when the record was 22–46?\n",
      "\n",
      "### Context\n",
      "The query will run on a database with the following schema:\n",
      "CREATE TABLE table_name_36 (score VARCHAR, record VARCHAR)\n",
      "\n",
      "--------------------\n",
      "SELECT score FROM table_name_36 WHERE record = \"22–46\"\n",
      "--------------------\n",
      "SELECT score FROM table_name_36 WHERE record = \"22–46\"\"\"\"\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "PROMPT_DICT = \"\"\"\n",
    "### Question\n",
    "Write an SQL query that answers this question:\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "The query will run on a database with the following schema:\n",
    "{context}\n",
    "\"\"\"\n",
    "q1 = {\"nl\": \"\\n### Question\\nWrite an SQL query that answers this question:\\nWhat was the score of the game when the record was 22\\u201346?\\n\\n### Context\\nThe query will run on a database with the following schema:\\nCREATE TABLE table_name_36 (score VARCHAR, record VARCHAR)\\n\", \"code\": \"SELECT score FROM table_name_36 WHERE record = \\\"22\\u201346\\\"\"}\n",
    "\n",
    "print(q1[\"nl\"])\n",
    "print(\"-\"*20)\n",
    "print(q1[\"code\"])\n",
    "print(\"-\"*20)\n",
    "print(Baseline.generate(q1[\"nl\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
