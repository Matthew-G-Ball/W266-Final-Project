{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 04:04:29.318607: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-13 04:04:29.340229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-13 04:04:29.340272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-13 04:04:29.340821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-13 04:04:29.344635: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 04:04:30.017451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from _utils import *\n",
    "from Model import Model_Class\n",
    "import evaluate\n",
    "import re\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T5 w/o Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'concode',\n",
       " 'always_save_model': False,\n",
       " 'save_last_checkpoints': False,\n",
       " 'cache_path': 'cache_data',\n",
       " 'load_model_path': None,\n",
       " 'config_name': 'Salesforce/codet5-base',\n",
       " 'model_name': 't5-base',\n",
       " 'tokenizer_name': 'Salesforce/codet5-base',\n",
       " 'dev_filename': 'validation_filtered.json',\n",
       " 'train_filename': 'train_filtered.json',\n",
       " 'test_filename': 'None',\n",
       " 'lang': 'SQL',\n",
       " 'data_dir': 'data',\n",
       " 'output_dir': 'saved_models/',\n",
       " 'summary_dir': 'summary',\n",
       " 'res_dir': 'res',\n",
       " 'cpu_cont': 1,\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'summary_verbose': False,\n",
       " 'log_verbose': True,\n",
       " 'info_level': 1,\n",
       " 'is_sample': False,\n",
       " 'sample_size': 5000,\n",
       " 'seed': 1234,\n",
       " 'learning_rate': '5e-05',\n",
       " 'start_epoch': 0,\n",
       " 'num_train_epochs': 5,\n",
       " 'train_batch_size': 64,\n",
       " 'eval_batch_size': 64,\n",
       " 'adam_epsilon': '1e-08',\n",
       " 'max_source_length': 100,\n",
       " 'max_target_length': 50,\n",
       " 'beam_size': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'data_num': -1,\n",
       " 'patience': 5,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5_config = yaml.load(open(\"configs/T5_config.yml\"),Loader=yaml.FullLoader)\n",
    "T5_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/13/2024 03:48:21 - INFO - Model -   Finish loading model [223M] from t5-base\n"
     ]
    }
   ],
   "source": [
    "T5_Model = Model_Class(T5_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:   0%|          | 0/4443 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 4443/4443 [34:25<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "prediction = []\n",
    "with open(\"data/testing_filtered.json\") as f:\n",
    "    for line in tqdm(f.readlines(),desc=\"Generating Predictions\"):\n",
    "        q1 = json.loads(line)\n",
    "        prediction.append(T5_Model.generate(q1[\"nl\"]))\n",
    "        answer.append(q1[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/13/2024 03:40:28 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=prediction,\n",
    "                        references=answer)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 23:14:59 - INFO - Model -   Load cache data from cache_data/train_all.pt\n",
      "/home/matt/anaconda3/envs/NLP/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "04/12/2024 23:15:00 - INFO - Model -   ***** Running training *****\n",
      "04/12/2024 23:15:00 - INFO - Model -     Num examples = 272628\n",
      "04/12/2024 23:15:00 - INFO - Model -     Batch size = 64\n",
      "04/12/2024 23:15:00 - INFO - Model -     Batch num = 4260\n",
      "04/12/2024 23:15:00 - INFO - Model -     Num epoch = 5\n",
      "[0] Train loss 1.352: 100%|██████████| 4260/4260 [17:12<00:00,  4.13it/s]\n",
      "04/12/2024 23:32:12 - INFO - Model -   Load cache data from cache_data/dev_all.pt\n",
      "04/12/2024 23:32:12 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/12/2024 23:32:12 - INFO - Model -     Num examples = 74118\n",
      "04/12/2024 23:32:12 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:20<00:00, 13.30it/s]\n",
      "04/12/2024 23:33:32 - INFO - Model -     epoch = 0\n",
      "04/12/2024 23:33:32 - INFO - Model -     eval_ppl = 2.27088\n",
      "04/12/2024 23:33:32 - INFO - Model -     global_step = 4260\n",
      "04/12/2024 23:33:32 - INFO - Model -     ********************\n",
      "04/12/2024 23:33:32 - INFO - Model -     Best ppl:2.27088\n",
      "04/12/2024 23:33:32 - INFO - Model -     ********************\n",
      "04/12/2024 23:33:32 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[1] Train loss 0.656: 100%|██████████| 4260/4260 [17:12<00:00,  4.12it/s]\n",
      "04/12/2024 23:50:45 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/12/2024 23:50:45 - INFO - Model -     Num examples = 74118\n",
      "04/12/2024 23:50:45 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:20<00:00, 13.18it/s]\n",
      "04/12/2024 23:52:06 - INFO - Model -     epoch = 1\n",
      "04/12/2024 23:52:06 - INFO - Model -     eval_ppl = 1.84382\n",
      "04/12/2024 23:52:06 - INFO - Model -     global_step = 8520\n",
      "04/12/2024 23:52:06 - INFO - Model -     ********************\n",
      "04/12/2024 23:52:06 - INFO - Model -     Best ppl:1.84382\n",
      "04/12/2024 23:52:06 - INFO - Model -     ********************\n",
      "04/12/2024 23:52:06 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[2] Train loss 0.528: 100%|██████████| 4260/4260 [17:53<00:00,  3.97it/s]\n",
      "04/13/2024 00:10:00 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/13/2024 00:10:00 - INFO - Model -     Num examples = 74118\n",
      "04/13/2024 00:10:00 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:22<00:00, 12.89it/s]\n",
      "04/13/2024 00:11:22 - INFO - Model -     epoch = 2\n",
      "04/13/2024 00:11:22 - INFO - Model -     eval_ppl = 1.70021\n",
      "04/13/2024 00:11:22 - INFO - Model -     global_step = 12780\n",
      "04/13/2024 00:11:22 - INFO - Model -     ********************\n",
      "04/13/2024 00:11:22 - INFO - Model -     Best ppl:1.70021\n",
      "04/13/2024 00:11:22 - INFO - Model -     ********************\n",
      "04/13/2024 00:11:22 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[3] Train loss 0.468: 100%|██████████| 4260/4260 [17:00<00:00,  4.17it/s]\n",
      "04/13/2024 00:28:23 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/13/2024 00:28:23 - INFO - Model -     Num examples = 74118\n",
      "04/13/2024 00:28:23 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:17<00:00, 13.83it/s]\n",
      "04/13/2024 00:29:40 - INFO - Model -     epoch = 3\n",
      "04/13/2024 00:29:40 - INFO - Model -     eval_ppl = 1.63986\n",
      "04/13/2024 00:29:40 - INFO - Model -     global_step = 17040\n",
      "04/13/2024 00:29:40 - INFO - Model -     ********************\n",
      "04/13/2024 00:29:40 - INFO - Model -     Best ppl:1.63986\n",
      "04/13/2024 00:29:40 - INFO - Model -     ********************\n",
      "04/13/2024 00:29:40 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[4] Train loss 0.442: 100%|██████████| 4260/4260 [16:18<00:00,  4.35it/s]\n",
      "04/13/2024 00:45:58 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/13/2024 00:45:58 - INFO - Model -     Num examples = 74118\n",
      "04/13/2024 00:45:58 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:16<00:00, 13.85it/s]\n",
      "04/13/2024 00:47:15 - INFO - Model -     epoch = 4\n",
      "04/13/2024 00:47:15 - INFO - Model -     eval_ppl = 1.63426\n",
      "04/13/2024 00:47:15 - INFO - Model -     global_step = 21300\n",
      "04/13/2024 00:47:15 - INFO - Model -     ********************\n",
      "04/13/2024 00:47:15 - INFO - Model -     Best ppl:1.63426\n",
      "04/13/2024 00:47:15 - INFO - Model -     ********************\n",
      "04/13/2024 00:47:15 - INFO - Model -   ***** CUDA.empty_cache() *****\n"
     ]
    }
   ],
   "source": [
    "T5_Model.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Baseline w/o Training Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'concode',\n",
       " 'always_save_model': False,\n",
       " 'save_last_checkpoints': False,\n",
       " 'cache_path': 'cache_data',\n",
       " 'load_model_path': None,\n",
       " 'config_name': 'Salesforce/codet5-base',\n",
       " 'model_name': 'Salesforce/codet5-base',\n",
       " 'tokenizer_name': 'Salesforce/codet5-base',\n",
       " 'dev_filename': 'validation_filtered.json',\n",
       " 'train_filename': 'train_filtered.json',\n",
       " 'test_filename': 'None',\n",
       " 'lang': 'SQL',\n",
       " 'data_dir': 'data',\n",
       " 'output_dir': 'saved_models/',\n",
       " 'summary_dir': 'summary',\n",
       " 'res_dir': 'res',\n",
       " 'cpu_cont': 1,\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'summary_verbose': False,\n",
       " 'log_verbose': True,\n",
       " 'info_level': 1,\n",
       " 'is_sample': False,\n",
       " 'sample_size': 5000,\n",
       " 'seed': 1234,\n",
       " 'learning_rate': '5e-05',\n",
       " 'start_epoch': 0,\n",
       " 'num_train_epochs': 5,\n",
       " 'train_batch_size': 64,\n",
       " 'eval_batch_size': 64,\n",
       " 'adam_epsilon': '1e-08',\n",
       " 'max_source_length': 100,\n",
       " 'max_target_length': 50,\n",
       " 'beam_size': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'data_num': -1,\n",
       " 'patience': 5,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base_config = yaml.load(open(\"configs/Base_config.yml\"),Loader=yaml.FullLoader)\n",
    "Base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Baseline w/o Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_Model = Model_Class(Base_config)\n",
    "answer = []\n",
    "prediction = []\n",
    "with open(\"data/testing_filtered.json\") as f:\n",
    "    for line in tqdm(f.readlines(),desc=\"Generating Predictions\"):\n",
    "        q1 = json.loads(line)\n",
    "        prediction.append(Base_Model.generate(q1[\"nl\"]))\n",
    "        answer.append(q1[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/06/2024 16:41:53 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.15734154154202606, 'rouge2': 0.05234838613042284, 'rougeL': 0.14547830785925475, 'rougeLsum': 0.14561331610868022}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=prediction,\n",
    "                        references=answer)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Baseline w/ Training Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'concode',\n",
       " 'always_save_model': True,\n",
       " 'save_last_checkpoints': False,\n",
       " 'cache_path': 'cache_data',\n",
       " 'load_model_path': 'saved_models/Baseline/Baseline_Best.bin',\n",
       " 'config_name': 'Salesforce/codet5-base',\n",
       " 'model_name': 'Salesforce/codet5-base',\n",
       " 'tokenizer_name': 'Salesforce/codet5-base',\n",
       " 'dev_filename': 'validation_filtered.json',\n",
       " 'train_filename': 'train_filtered.json',\n",
       " 'test_filename': 'None',\n",
       " 'lang': 'SQL',\n",
       " 'data_dir': 'data',\n",
       " 'output_dir': 'saved_models/',\n",
       " 'summary_dir': 'summary',\n",
       " 'res_dir': 'res',\n",
       " 'cpu_cont': 1,\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'summary_verbose': False,\n",
       " 'log_verbose': True,\n",
       " 'info_level': 1,\n",
       " 'is_sample': False,\n",
       " 'sample_size': 5000,\n",
       " 'seed': 1234,\n",
       " 'learning_rate': '5e-05',\n",
       " 'start_epoch': 0,\n",
       " 'num_train_epochs': 5,\n",
       " 'train_batch_size': 64,\n",
       " 'eval_batch_size': 64,\n",
       " 'adam_epsilon': '1e-08',\n",
       " 'max_source_length': 100,\n",
       " 'max_target_length': 50,\n",
       " 'beam_size': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'data_num': -1,\n",
       " 'patience': 5,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = yaml.load(open(\"configs/config.yml\"),Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load & Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/envs/NLP/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "04/06/2024 07:42:22 - INFO - Model -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "04/06/2024 07:42:22 - INFO - Model -   Reload model from saved_models/Baseline/Base.bin\n",
      "04/06/2024 07:42:34 - INFO - Model -   Load cache data from cache_data/train_all.pt\n",
      "/home/matt/anaconda3/envs/NLP/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "04/06/2024 07:42:34 - INFO - Model -   ***** Running training *****\n",
      "04/06/2024 07:42:34 - INFO - Model -     Num examples = 272628\n",
      "04/06/2024 07:42:34 - INFO - Model -     Batch size = 64\n",
      "04/06/2024 07:42:34 - INFO - Model -     Batch num = 4260\n",
      "04/06/2024 07:42:34 - INFO - Model -     Num epoch = 5\n",
      "[0] Train loss 0.056: 100%|██████████| 4260/4260 [16:39<00:00,  4.26it/s]\n",
      "04/06/2024 07:59:14 - INFO - Model -   Load cache data from cache_data/dev_all.pt\n",
      "04/06/2024 07:59:14 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/06/2024 07:59:14 - INFO - Model -     Num examples = 39221\n",
      "04/06/2024 07:59:14 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:19<00:00, 13.45it/s]\n",
      "04/06/2024 08:00:33 - INFO - Model -     epoch = 0\n",
      "04/06/2024 08:00:33 - INFO - Model -     eval_ppl = 1.0863\n",
      "04/06/2024 08:00:33 - INFO - Model -     global_step = 4260\n",
      "04/06/2024 08:00:33 - INFO - Model -     ********************\n",
      "04/06/2024 08:00:33 - INFO - Model -     Best ppl:1.0863\n",
      "04/06/2024 08:00:33 - INFO - Model -     ********************\n",
      "04/06/2024 08:00:34 - INFO - Model -   Save the best ppl model into saved_models/checkpoint-best-ppl/pytorch_model.bin\n",
      "04/06/2024 08:00:34 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[1] Train loss 0.045: 100%|██████████| 4260/4260 [16:47<00:00,  4.23it/s]\n",
      "04/06/2024 08:17:22 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/06/2024 08:17:22 - INFO - Model -     Num examples = 39221\n",
      "04/06/2024 08:17:22 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:19<00:00, 13.48it/s]\n",
      "04/06/2024 08:18:41 - INFO - Model -     epoch = 1\n",
      "04/06/2024 08:18:41 - INFO - Model -     eval_ppl = 1.11276\n",
      "04/06/2024 08:18:41 - INFO - Model -     global_step = 8520\n",
      "04/06/2024 08:18:41 - INFO - Model -     ********************\n",
      "04/06/2024 08:18:41 - INFO - Model -   Ppl does not decrease for 1 epochs\n",
      "04/06/2024 08:18:41 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[2] Train loss 0.037: 100%|██████████| 4260/4260 [16:53<00:00,  4.20it/s]\n",
      "04/06/2024 08:35:35 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/06/2024 08:35:35 - INFO - Model -     Num examples = 39221\n",
      "04/06/2024 08:35:35 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:20<00:00, 13.28it/s]\n",
      "04/06/2024 08:36:55 - INFO - Model -     epoch = 2\n",
      "04/06/2024 08:36:55 - INFO - Model -     eval_ppl = 1.17327\n",
      "04/06/2024 08:36:55 - INFO - Model -     global_step = 12780\n",
      "04/06/2024 08:36:55 - INFO - Model -     ********************\n",
      "04/06/2024 08:36:55 - INFO - Model -   Ppl does not decrease for 2 epochs\n",
      "04/06/2024 08:36:55 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[3] Train loss 0.032: 100%|██████████| 4260/4260 [17:06<00:00,  4.15it/s]\n",
      "04/06/2024 08:54:02 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/06/2024 08:54:02 - INFO - Model -     Num examples = 39221\n",
      "04/06/2024 08:54:02 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:17<00:00, 13.72it/s]\n",
      "04/06/2024 08:55:19 - INFO - Model -     epoch = 3\n",
      "04/06/2024 08:55:19 - INFO - Model -     eval_ppl = 1.13805\n",
      "04/06/2024 08:55:19 - INFO - Model -     global_step = 17040\n",
      "04/06/2024 08:55:19 - INFO - Model -     ********************\n",
      "04/06/2024 08:55:19 - INFO - Model -   Ppl does not decrease for 3 epochs\n",
      "04/06/2024 08:55:19 - INFO - Model -   ***** CUDA.empty_cache() *****\n",
      "[4] Train loss 0.028: 100%|██████████| 4260/4260 [16:29<00:00,  4.31it/s]\n",
      "04/06/2024 09:11:49 - INFO - Model -     ***** Running ppl evaluation *****\n",
      "04/06/2024 09:11:49 - INFO - Model -     Num examples = 39221\n",
      "04/06/2024 09:11:49 - INFO - Model -     Batch size = 64\n",
      "Eval ppl: 100%|██████████| 1065/1065 [01:17<00:00, 13.73it/s]\n",
      "04/06/2024 09:13:06 - INFO - Model -     epoch = 4\n",
      "04/06/2024 09:13:06 - INFO - Model -     eval_ppl = 1.09659\n",
      "04/06/2024 09:13:06 - INFO - Model -     global_step = 21300\n",
      "04/06/2024 09:13:06 - INFO - Model -     ********************\n",
      "04/06/2024 09:13:06 - INFO - Model -   Ppl does not decrease for 4 epochs\n",
      "04/06/2024 09:13:06 - INFO - Model -   ***** CUDA.empty_cache() *****\n"
     ]
    }
   ],
   "source": [
    "Model = Model_Class(config)\n",
    "train_base = input(\"Would you like to train the Base_Model (y/n): \")\n",
    "if train_base == 'y':\n",
    "    Model.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Baseline w/ Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "prediction = []\n",
    "with open(\"data/testing_filtered.json\") as f:\n",
    "    for line in tqdm(f.readlines(),desc=\"Generating Predictions\"):\n",
    "        q1 = json.loads(line)\n",
    "        prediction.append(Model.generate(q1[\"nl\"]))\n",
    "        answer.append(q1[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/06/2024 15:50:48 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.8636640033742007, 'rouge2': 0.8019490870939264, 'rougeL': 0.8500682335775959, 'rougeLsum': 0.8498248334695415}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=prediction,\n",
    "                        references=answer)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup and Apply Dora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'concode',\n",
       " 'always_save_model': True,\n",
       " 'save_last_checkpoints': False,\n",
       " 'cache_path': 'cache_data',\n",
       " 'load_model_path': None,\n",
       " 'config_name': 'Salesforce/codet5-base',\n",
       " 'model_name': 'Salesforce/codet5-base',\n",
       " 'tokenizer_name': 'Salesforce/codet5-base',\n",
       " 'dev_filename': 'validation_filtered.json',\n",
       " 'train_filename': 'train_filtered.json',\n",
       " 'test_filename': 'None',\n",
       " 'lang': 'SQL',\n",
       " 'data_dir': 'data',\n",
       " 'output_dir': 'saved_models/',\n",
       " 'summary_dir': 'summary',\n",
       " 'res_dir': 'res',\n",
       " 'cpu_cont': 1,\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'summary_verbose': False,\n",
       " 'log_verbose': True,\n",
       " 'info_level': 1,\n",
       " 'is_sample': False,\n",
       " 'sample_size': 5000,\n",
       " 'seed': 1234,\n",
       " 'learning_rate': '5e-05',\n",
       " 'start_epoch': 0,\n",
       " 'num_train_epochs': 20,\n",
       " 'train_batch_size': 64,\n",
       " 'eval_batch_size': 64,\n",
       " 'adam_epsilon': '1e-08',\n",
       " 'max_source_length': 100,\n",
       " 'max_target_length': 50,\n",
       " 'beam_size': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'data_num': -1,\n",
       " 'patience': 5,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dora_Config = yaml.load(open(\"configs/Dora_config.yml\"),Loader=yaml.FullLoader)\n",
    "Dora_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/envs/NLP/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "04/13/2024 03:57:57 - INFO - Model -   Finish loading model [223M] from Salesforce/codet5-base\n"
     ]
    }
   ],
   "source": [
    "Dora_Model = Model_Class(Dora_Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "trainable params: 251248896 || all params: 251248896 || trainable%: 100\n",
      "--------------------------------------------------------------------------------\n",
      "Model w/ Dora:\n",
      "trainable params: 28,366,848 || all params: 251,248,896 || trainable%: 11.290337371273464\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# Hyperparameters\n",
    "r = 256\n",
    "lora_alpha = 512 #2 times r\n",
    "lora_dropout = 0.1\n",
    "temp_model = Dora_Model.model\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, \n",
    "                         inference_mode=False, \n",
    "                         r=r, \n",
    "                         lora_alpha=lora_alpha, \n",
    "                         lora_dropout=lora_dropout,\n",
    "                         use_dora=True)\n",
    "\n",
    "temp_model = get_peft_model(temp_model, peft_config)\n",
    "print(\"Original:\")\n",
    "total_param = sum(p.numel() for p in Dora_Model.model.parameters())\n",
    "print(f\"trainable params: {total_param} || all params: {total_param} || trainable%: 100\")\n",
    "print(\"-\"*80)\n",
    "print(\"Model w/ Dora:\")\n",
    "temp_model.print_trainable_parameters()\n",
    "\n",
    "Dora_Model.set_model(temp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Dora Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp = input(\"Would you like to train the new Model (y/n): \")\n",
    "if train_exp == 'y':\n",
    "    Dora_Model.training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Dora Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:   0%|          | 0/4443 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 4443/4443 [42:18<00:00,  1.75it/s]\n",
      "04/08/2024 05:11:48 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.822827901409901, 'rouge2': 0.7356028427074228, 'rougeL': 0.8017589341657259, 'rougeLsum': 0.8017071706924369}\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "prediction = []\n",
    "with open(\"data/testing_filtered.json\") as f:\n",
    "    for line in tqdm(f.readlines(),desc=\"Generating Predictions\"):\n",
    "        q1 = json.loads(line)\n",
    "        prediction.append(Dora_Model.generate(q1[\"nl\"]))\n",
    "        answer.append(q1[\"code\"])\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=prediction,\n",
    "                        references=answer)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make 3 different training datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Masked Span Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:00<00:00, 569715.60it/s]\n"
     ]
    }
   ],
   "source": [
    "Masked_span = []\n",
    "with open(\"data/train_filtered.json\",\"r\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        code = json.loads(line)['code']\n",
    "        Masked_span.append({\"nl\": code, \"code\": code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:04<00:00, 63948.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "for x in tqdm(Masked_span):\n",
    "    out_list = re.split('([^a-zA-Z0-9_])', x[\"nl\"])\n",
    "\n",
    "    num_mask = np.random.randint(1, 2+int(len(out_list)/4))\n",
    "    mask_values = np.random.choice(range(len(out_list)),num_mask,replace=False)\n",
    "\n",
    "    for i, word in enumerate([out_list[i] for i in mask_values]):\n",
    "        if len(word) > 3:\n",
    "            out_list[out_list.index(word)] = f\"Mask{i}\"\n",
    "    x[\"nl\"] = ''.join(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309887/309887 [00:01<00:00, 197771.36it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 178885.99it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/training_Masked_Span_Prediction.json\", \"w\") as f:\n",
    "    for x in tqdm(Masked_span[:-5000]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"data/validation_training_Masked_Span_Prediction.json\", \"w\") as f:\n",
    "    for x in tqdm(Masked_span[-5000:]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Identifier Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:00<00:00, 546922.88it/s]\n"
     ]
    }
   ],
   "source": [
    "Identifier_Tagging = []\n",
    "with open(\"data/train_filtered.json\",\"r\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        code = json.loads(line)['code']\n",
    "        Identifier_Tagging.append({\"nl\": code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/314887 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:02<00:00, 110220.86it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"ADD\",\"CONSTRAINT\",\"ALL\",\"ALTER\",\"COLUMN\",\"TABLE\",\"AND\",\"ANY\",\"AS\",\"ASC\",\"BACKUP\",\"DATABASE\",\"BETWEEN\",\"CASE\",\"CHECK\",\"CREATE\",\"INDEX\",\"OR\",\"REPLACE\",\"VIEW\",\"PROCEDURE\",\"UNIQUE\",\"DEFAULT\",\"DELETE\",\"DESC\",\"DISTINCT\",\"DROP\",\"EXEC\",\"EXISTS\",\"FOREIGN\",\"KEY\",\"FROM\",\"FULL\",\"OUTER\",\"JOIN\",\"GROUP\",\"BY\",\"HAVING\",\"IN\",\"INNER\",\"INSERT\",\"INTO\",\"SELECT\",\"IS\",\"NULL\",\"NOT\",\"LEFT\",\"LIKE\",\"LIMIT\",\"ORDER\",\"PRIMARY\",\"RIGHT\",\"ROWNUM\",\"TOP\",\"SET\",\"TRUNCATE\",\"UNION\",\"ALL\",\"UPDATE\",\"VALUES\",\"WHERE\"]\n",
    "for i in tqdm(Identifier_Tagging):\n",
    "    temp_identifiers = re.split(\"\\\\W+\",i[\"nl\"])\n",
    "    out_list = re.split('([^a-zA-Z0-9_])', i[\"nl\"])\n",
    "\n",
    "    i[\"code\"] = [1 if x in temp_identifiers and x not in keywords else 0\n",
    "    for x in out_list if x.strip()]\n",
    "    i[\"code\"] = str(i[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309887/309887 [00:01<00:00, 165677.93it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 201924.94it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/training_identifier_tagging.json\", \"w\") as f:\n",
    "    for x in tqdm(Identifier_Tagging[:-5000]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"data/validation_training_identifier_tagging.json\", \"w\") as f:\n",
    "    for x in tqdm(Identifier_Tagging[-5000:]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Masked Identifier Prediction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:00<00:00, 543970.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"training_Masked_Identifier_Prediciton.json\")\n",
    "Masked_Identifier_Prompt = \"{nl}\\n### Masked:\\n{code}\"\n",
    "Masked_Identifier_Tagging = []\n",
    "with open(\"data/train_filtered.json\",\"r\") as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        Masked_Identifier_Tagging.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314887/314887 [00:04<00:00, 75078.08it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"ADD\",\"CONSTRAINT\",\"ALL\",\"ALTER\",\"COLUMN\",\"TABLE\",\"AND\",\"ANY\",\"AS\",\"ASC\",\"BACKUP\",\"DATABASE\",\"BETWEEN\",\"CASE\",\"CHECK\",\"CREATE\",\"INDEX\",\"OR\",\"REPLACE\",\"VIEW\",\"PROCEDURE\",\"UNIQUE\",\"DEFAULT\",\"DELETE\",\"DESC\",\"DISTINCT\",\"DROP\",\"EXEC\",\"EXISTS\",\"FOREIGN\",\"KEY\",\"FROM\",\"FULL\",\"OUTER\",\"JOIN\",\"GROUP\",\"BY\",\"HAVING\",\"IN\",\"INNER\",\"INSERT\",\"INTO\",\"SELECT\",\"IS\",\"NULL\",\"NOT\",\"LEFT\",\"LIKE\",\"LIMIT\",\"ORDER\",\"PRIMARY\",\"RIGHT\",\"ROWNUM\",\"TOP\",\"SET\",\"TRUNCATE\",\"UNION\",\"ALL\",\"UPDATE\",\"VALUES\",\"WHERE\"]\n",
    "for i in tqdm(Masked_Identifier_Tagging):\n",
    "    temp_identifiers = re.split(\"\\\\W+\",i[\"code\"])\n",
    "    out_list = re.split('([^a-zA-Z0-9_])', i[\"code\"])\n",
    "    Masked_code = [f\"Mask{pos}\" if x in temp_identifiers and x not in keywords and x.strip() else x\n",
    "    for pos, x in enumerate(out_list)]\n",
    "    Masked_code = \"\".join(i[\"code\"])\n",
    "    i[\"nl\"] = Masked_Identifier_Prompt.format_map({\"nl\":i[\"nl\"],\"code\":Masked_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309887/309887 [00:02<00:00, 135778.35it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 142720.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/training_Masked_Identifier_Prediciton.json\", \"w\") as f:\n",
    "    for x in tqdm(Masked_Identifier_Tagging[:-5000]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")\n",
    "with open(\"data/Validation_training_Masked_Identifier_Prediciton.json\", \"w\") as f:\n",
    "    for x in tqdm(Masked_Identifier_Tagging[-5000:]):\n",
    "        json.dump(x,f,indent=None,default=str)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'concode',\n",
       " 'always_save_model': True,\n",
       " 'save_last_checkpoints': False,\n",
       " 'cache_path': 'cache_data',\n",
       " 'load_model_path': None,\n",
       " 'config_name': 'Salesforce/codet5-base',\n",
       " 'model_name': 'Salesforce/codet5-base',\n",
       " 'tokenizer_name': 'Salesforce/codet5-base',\n",
       " 'dev_filename': 'validation_training_Masked_Span_Prediction.json',\n",
       " 'train_filename': 'training_Masked_Span_Prediction.json',\n",
       " 'test_filename': 'None',\n",
       " 'lang': 'SQL',\n",
       " 'data_dir': 'data',\n",
       " 'output_dir': 'saved_models/',\n",
       " 'summary_dir': 'summary',\n",
       " 'res_dir': 'res',\n",
       " 'cpu_cont': 1,\n",
       " 'device': 'cuda',\n",
       " 'do_eval': True,\n",
       " 'do_eval_bleu': False,\n",
       " 'do_test': True,\n",
       " 'do_train': True,\n",
       " 'summary_verbose': False,\n",
       " 'log_verbose': True,\n",
       " 'info_level': 1,\n",
       " 'is_sample': False,\n",
       " 'sample_size': 5000,\n",
       " 'seed': 1234,\n",
       " 'learning_rate': '5e-05',\n",
       " 'start_epoch': 0,\n",
       " 'num_train_epochs': 5,\n",
       " 'train_batch_size': 64,\n",
       " 'eval_batch_size': 64,\n",
       " 'adam_epsilon': '1e-08',\n",
       " 'max_source_length': 100,\n",
       " 'max_target_length': 50,\n",
       " 'beam_size': 1,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'data_num': -1,\n",
       " 'patience': 5,\n",
       " 'decoder_start_token_id': 1,\n",
       " 'warmup_steps': 100,\n",
       " 'weight_decay': 0.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Config = yaml.load(open(\"configs/Experiment_config.yml\"),Loader=yaml.FullLoader)\n",
    "Experiment_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/envs/NLP/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "04/13/2024 04:04:35 - INFO - Model -   Finish loading model [223M] from Salesforce/codet5-base\n"
     ]
    }
   ],
   "source": [
    "Experiment_Model = Model_Class(Experiment_Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "trainable params: 251248896 || all params: 251248896 || trainable%: 100\n",
      "--------------------------------------------------------------------------------\n",
      "Model w/ Dora:\n",
      "trainable params: 28,366,848 || all params: 251,248,896 || trainable%: 11.290337371273464\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# Hyperparameters\n",
    "r = 256\n",
    "lora_alpha = 512 #2 times r\n",
    "lora_dropout = 0.1\n",
    "temp_model = Experiment_Model.model\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, \n",
    "                         inference_mode=False, \n",
    "                         r=r, \n",
    "                         lora_alpha=lora_alpha, \n",
    "                         lora_dropout=lora_dropout,\n",
    "                         use_dora=True)\n",
    "\n",
    "temp_model = get_peft_model(temp_model, peft_config)\n",
    "print(\"Original:\")\n",
    "total_param = sum(p.numel() for p in Experiment_Model.model.parameters())\n",
    "print(f\"trainable params: {total_param} || all params: {total_param} || trainable%: 100\")\n",
    "print(\"-\"*80)\n",
    "print(\"Model w/ Dora:\")\n",
    "temp_model.print_trainable_parameters()\n",
    "\n",
    "Experiment_Model.set_model(temp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Masked Span Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp = input(\"Would you like to train the new Model (y/n): \")\n",
    "if train_exp == 'y':\n",
    "    Experiment_Model.training_loop(split_tags=[\"train_MSP_wo_Dora\",\"dev_MSP_wo_Dora\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 03:20:24 - INFO - Model -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "04/10/2024 03:20:24 - INFO - Model -   Reload model from saved_models/msp_wo_dora_model.bin\n"
     ]
    }
   ],
   "source": [
    "#Load Saved\n",
    "Experiment_Model.load_model(model=Experiment_Model.model, path=\"saved_models/msp_wo_dora_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifier Tagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the model so that the last layer of the encoder transformer heads are provided as output from the model.\n",
    "\n",
    "Attach the encoder heads to the desired task / loss.\n",
    "\n",
    "Train the modified model using the new loss based only on the encoder heads and not any part of the decoder, providoing task input text to the encoder and (if they really want to minimize code changes) pass an empty sequence to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change training data\n",
    "Experiment_Model.__setattr__(\"train_filename\",\"data/training_identifier_tagging.json\")\n",
    "Experiment_Model.__setattr__(\"dev_filename\",\"data/validation_training_identifier_tagging.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_weights = []\n",
    "for x in Experiment_Model.model.decoder.parameters():\n",
    "    x.requires_grad = False\n",
    "    list_of_weights.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.training_loop(split_tags=[\"train_it_wo_dora\",\"dev_it_wo_dora\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new_list_of_weights = []\n",
    "for x in Experiment_Model.model.decoder.parameters():\n",
    "    new_list_of_weights.append(x)\n",
    "    x.requires_grad = True\n",
    "\n",
    "print(list_of_weights == new_list_of_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2024 04:11:47 - INFO - Model -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "04/10/2024 04:11:47 - INFO - Model -   Reload model from saved_models/v2_it_model_wo_dora.bin\n"
     ]
    }
   ],
   "source": [
    "# Experiment_Model.save_model_state(\"v2_it_model_wo_dora.bin\")\n",
    "# Experiment_Model.load_model(model=Experiment_Model.model, path=\"saved_models/v2_it_model_wo_dora.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masked Identifier Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.__setattr__(\"train_filename\",\"data/training_Masked_Identifier_Prediciton.json\")\n",
    "Experiment_Model.__setattr__(\"dev_filename\",\"data/Validation_training_Masked_Identifier_Prediciton.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.training_loop(split_tags=[\"train_mip_wo_dora\",\"dev_mip_wo_dora\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train on default data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.__setattr__(\"dev_filename\",\"data/train_filtered.json\")\n",
    "Experiment_Model.__setattr__(\"train_filename\",\"data/validation_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Model.save_model_state(\"final_experiment_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 4443/4443 [24:21<00:00,  3.04it/s]\n",
      "04/10/2024 06:33:41 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.8947285696912889, 'rouge2': 0.8410073084335176, 'rougeL': 0.8814300390326623, 'rougeLsum': 0.8812268044482586}\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "prediction = []\n",
    "with open(\"data/testing_filtered.json\") as f:\n",
    "    for line in tqdm(f.readlines(),desc=\"Generating Predictions\"):\n",
    "        q1 = json.loads(line)\n",
    "        prediction.append(Experiment_Model.generate(q1[\"nl\"]))\n",
    "        answer.append(q1[\"code\"])\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=prediction,\n",
    "                        references=answer)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import CHRFScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1609)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrf = CHRFScore(n_char_order=1, \n",
    "                 n_word_order=2,\n",
    "                 lowercase=True,\n",
    "                 whitespace=False)\n",
    "chrf(Model.generate(q1[\"nl\"]),q1[\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ### Question\n",
      "    Write an SQL query that answers this question:\n",
      "    What is the highest Byes by Anakie who has an Against smaller than 2275?\n",
      "\n",
      "    ### Context\n",
      "    The query will run on a database with the following schema:\n",
      "    CREATE TABLE table_name_14 (\n",
      "    byes INTEGER,\n",
      "    geelong_dfl VARCHAR,\n",
      "    against VARCHAR\n",
      ")\n",
      "    \n",
      "--------------------\n",
      "SELECT MAX(byes) FROM table_name_14 WHERE geelong_dfl = \"anakie\" AND against < 2275\n",
      "--------------------\n",
      "SELECT MAX(byes) FROM table_name_14 WHERE geelong_dfl = \"anakie\" AND against < 2275\n"
     ]
    }
   ],
   "source": [
    "PROMPT_DICT = \"\"\"\n",
    "### Question\n",
    "Write an SQL query that answers this question:\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "The query will run on a database with the following schema:\n",
    "{context}\n",
    "\"\"\"\n",
    "q1 = {\"nl\": \"\\n    ### Question\\n    Write an SQL query that answers this question:\\n    What is the highest Byes by Anakie who has an Against smaller than 2275?\\n\\n    ### Context\\n    The query will run on a database with the following schema:\\n    CREATE TABLE table_name_14 (\\n    byes INTEGER,\\n    geelong_dfl VARCHAR,\\n    against VARCHAR\\n)\\n    \", \"code\": \"SELECT MAX(byes) FROM table_name_14 WHERE geelong_dfl = \\\"anakie\\\" AND against < 2275\"}\n",
    "\n",
    "print(q1[\"nl\"])\n",
    "print(\"-\"*20)\n",
    "print(q1[\"code\"])\n",
    "print(\"-\"*20)\n",
    "print(Model.generate(q1[\"nl\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
